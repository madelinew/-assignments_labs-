{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "=======================================================================\n",
    "Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n",
    "=======================================================================\n",
    "\n",
    "When working with covariance estimation, the usual approach is to use\n",
    "a maximum likelihood estimator, such as the\n",
    ":class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it\n",
    "converges to the true (population) covariance when given many\n",
    "observations. However, it can also be beneficial to regularize it, in\n",
    "order to reduce its variance; this, in turn, introduces some bias. This\n",
    "example illustrates the simple regularization used in\n",
    "`shrunk_covariance` estimators. In particular, it focuses on how to\n",
    "set the amount of regularization, i.e. how to choose the bias-variance\n",
    "trade-off.\n",
    "\n",
    "Here we compare 3 approaches:\n",
    "\n",
    "* Setting the parameter by cross-validating the likelihood on three folds\n",
    "  according to a grid of potential shrinkage parameters.\n",
    "\n",
    "* A close formula proposed by Ledoit and Wolf to compute\n",
    "  the asymptotically optimal regularization parameter (minimizing a MSE\n",
    "  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`\n",
    "  covariance estimate.\n",
    "\n",
    "* An improvement of the Ledoit-Wolf shrinkage, the\n",
    "  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its\n",
    "  convergence is significantly better under the assumption that the data\n",
    "  are Gaussian, in particular for small samples.\n",
    "\n",
    "To quantify estimation error, we plot the likelihood of unseen data for\n",
    "different values of the shrinkage parameter. We also show the choices by\n",
    "cross-validation, or with the LedoitWolf and OAS estimates.\n",
    "\n",
    "Note that the maximum likelihood estimate corresponds to no shrinkage,\n",
    "and thus performs poorly. The Ledoit-Wolf estimate performs really well,\n",
    "as it is close to the optimal and is computational not costly. In this\n",
    "example, the OAS estimate is a bit further away. Interestingly, both\n",
    "approaches outperform cross-validation, which is significantly most\n",
    "computationally costly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\\n",
    "    log_likelihood, empirical_covariance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "n_features, n_samples = 40, 20\n",
    "np.random.seed(42)\n",
    "base_X_train = np.random.normal(size=(n_samples, n_features))\n",
    "base_X_test = np.random.normal(size=(n_samples, n_features))\n",
    "\n",
    "# Color samples\n",
    "coloring_matrix = np.random.normal(size=(n_features, n_features))\n",
    "X_train = np.dot(base_X_train, coloring_matrix)\n",
    "X_test = np.dot(base_X_test, coloring_matrix)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute the likelihood on test data\n",
    "\n",
    "# spanning a range of possible shrinkage coefficient values\n",
    "shrinkages = np.logspace(-2, 0, 30)\n",
    "negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n",
    "                    for s in shrinkages]\n",
    "\n",
    "# under the ground-truth model, which we would not have access to in real\n",
    "# settings\n",
    "real_cov = np.dot(coloring_matrix.T, coloring_matrix)\n",
    "emp_cov = empirical_covariance(X_train)\n",
    "loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n",
    "\n",
    "# #############################################################################\n",
    "# Compare different approaches to setting the parameter\n",
    "\n",
    "# GridSearch for an optimal shrinkage coefficient\n",
    "tuned_parameters = [{'shrinkage': shrinkages}]\n",
    "cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Ledoit-Wolf optimal shrinkage coefficient estimate\n",
    "lw = LedoitWolf()\n",
    "loglik_lw = lw.fit(X_train).score(X_test)\n",
    "\n",
    "# OAS coefficient estimate\n",
    "oa = OAS()\n",
    "loglik_oa = oa.fit(X_train).score(X_test)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot results\n",
    "fig = plt.figure()\n",
    "plt.title(\"Regularized covariance: likelihood and shrinkage coefficient\")\n",
    "plt.xlabel('Regularization parameter: shrinkage coefficient')\n",
    "plt.ylabel('Error: negative log-likelihood on test data')\n",
    "# range shrinkage curve\n",
    "plt.loglog(shrinkages, negative_logliks, label=\"Negative log-likelihood\")\n",
    "\n",
    "plt.plot(plt.xlim(), 2 * [loglik_real], '--r',\n",
    "         label=\"Real covariance likelihood\")\n",
    "\n",
    "# adjust view\n",
    "lik_max = np.amax(negative_logliks)\n",
    "lik_min = np.amin(negative_logliks)\n",
    "ymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\n",
    "ymax = lik_max + 10. * np.log(lik_max - lik_min)\n",
    "xmin = shrinkages[0]\n",
    "xmax = shrinkages[-1]\n",
    "# LW likelihood\n",
    "plt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',\n",
    "           linewidth=3, label='Ledoit-Wolf estimate')\n",
    "# OAS likelihood\n",
    "plt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',\n",
    "           linewidth=3, label='OAS estimate')\n",
    "# best CV estimator likelihood\n",
    "plt.vlines(cv.best_estimator_.shrinkage, ymin,\n",
    "           -cv.best_estimator_.score(X_test), color='cyan',\n",
    "           linewidth=3, label='Cross-validation best estimate')\n",
    "\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
